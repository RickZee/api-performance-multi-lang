-- Flink SQL Job for Event Filtering and Routing
-- This file is AUTO-GENERATED from filters.yaml
-- DO NOT EDIT MANUALLY - Changes will be overwritten
-- Generated by: generate-flink-sql.py
-- Source: filters.yaml

-- ============================================================================
-- Source Table: Raw Business Events from Kafka
-- ============================================================================
CREATE TABLE raw_business_events (
    `eventHeader` ROW<
        `uuid` STRING,
        `eventName` STRING,
        `createdDate` BIGINT,
        `savedDate` BIGINT,
        `eventType` STRING
    >,
    `eventBody` ROW<
        `entities` ARRAY<ROW<
            `entityType` STRING,
            `entityId` STRING,
            `updatedAttributes` MAP<STRING, STRING>
        >>
    >,
    `sourceMetadata` ROW<
        `table` STRING,
        `operation` STRING,
        `timestamp` BIGINT
    >,
    `proctime` AS PROCTIME(),
    `eventtime` AS TO_TIMESTAMP_LTZ(`eventHeader`.`createdDate`, 3),
    WATERMARK FOR `eventtime` AS `eventtime` - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'raw-business-events',
    'properties.bootstrap.servers' = '{{ kafka_bootstrap }}',
    'properties.group.id' = 'flink-routing-job',
    'format' = 'avro',
    'avro.schema-registry.url' = '{{ schema_registry_url }}',
    'scan.startup.mode' = 'earliest-offset'
);

{% for filter in filters %}
-- ============================================================================
-- Sink Table: {{ filter.name }}
-- ============================================================================
CREATE TABLE {{ filter.outputTopic|replace('-', '_') }} (
    `eventHeader` ROW<
        `uuid` STRING,
        `eventName` STRING,
        `createdDate` BIGINT,
        `savedDate` BIGINT,
        `eventType` STRING
    >,
    `eventBody` ROW<
        `entities` ARRAY<ROW<
            `entityType` STRING,
            `entityId` STRING,
            `updatedAttributes` MAP<STRING, STRING>
        >>
    >,
    `sourceMetadata` ROW<
        `table` STRING,
        `operation` STRING,
        `timestamp` BIGINT
    >,
    `filterMetadata` ROW<
        `filterId` STRING,
        `consumerId` STRING,
        `filteredAt` BIGINT
    >
) WITH (
    'connector' = 'kafka',
    'topic' = '{{ filter.outputTopic }}',
    'properties.bootstrap.servers' = '{{ kafka_bootstrap }}',
    'format' = 'avro',
    'avro.schema-registry.url' = '{{ schema_registry_url }}',
    'sink.partitioner' = 'fixed'
);

{% endfor %}
-- ============================================================================
-- Filtering and Routing Queries
-- ============================================================================

{% for filter in filters %}
-- {{ filter.name }}
-- {% if filter.description %}{{ filter.description }}{% endif %}
INSERT INTO {{ filter.outputTopic|replace('-', '_') }}
SELECT 
    `eventHeader`,
    `eventBody`,
    `sourceMetadata`,
    ROW(
        '{{ filter.id }}',
        '{{ filter.consumerId }}',
        UNIX_TIMESTAMP() * 1000
    ) AS `filterMetadata`
FROM raw_business_events
{% if filter.conditions %}
WHERE {{ generator.generate_where_clause(filter) }}
{% endif %};

{% endfor %}





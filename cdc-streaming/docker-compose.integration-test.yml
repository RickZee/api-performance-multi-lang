version: '3.8'

services:
  # Note: Using Confluent Cloud for Kafka and Schema Registry
  # No local Kafka or Schema Registry containers needed

  # PostgreSQL database for Metadata Service
  # Note: Metadata Service uses local postgres for integration tests
  # CDC pipeline/producer APIs use Aurora (configured via .env.aurora)
  postgres:
    image: postgres:15-alpine
    container_name: int-test-postgres
    environment:
      POSTGRES_DB: metadata_service
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      retries: 10
    volumes:
      - postgres-data:/var/lib/postgresql/data
    profiles:
      - local-db  # Only start local postgres if not using Aurora

  # Mock Confluent Flink API (for deployment testing)
  mock-confluent-api:
    build:
      context: ./test-infrastructure/mock-confluent-api
      dockerfile: Dockerfile
    container_name: int-test-mock-confluent-api
    ports:
      - "8082:8082"
    environment:
      MOCK_API_PORT: 8082

  # Metadata Service (uses mock Confluent API)
  metadata-service:
    build:
      context: ../metadata-service-java
      dockerfile: Dockerfile
    container_name: int-test-metadata-service
    depends_on:
      postgres:
        condition: service_healthy
        required: false  # Not required if using Aurora
      mock-confluent-api:
        condition: service_started
    ports:
      - "8080:8080"
    environment:
      GIT_REPOSITORY: file:///app/test-data
      CONFLUENT_CLOUD_API_ENDPOINT: http://mock-confluent-api:8082
      CONFLUENT_CLOUD_API_KEY: test-key
      CONFLUENT_CLOUD_API_SECRET: test-secret
      # Database configuration - use Aurora if available, otherwise local postgres
      DATABASE_URL: ${DATABASE_URL:-jdbc:postgresql://postgres:5432/metadata_service}
      DATABASE_USERNAME: ${DATABASE_USERNAME:-postgres}
      DATABASE_PASSWORD: ${DATABASE_PASSWORD:-postgres}
      # Confluent Cloud configuration (from environment variables)
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_API_KEY: ${KAFKA_API_KEY:-}
      KAFKA_API_SECRET: ${KAFKA_API_SECRET:-}
    volumes:
      - ../data:/app/test-data:ro
    healthcheck:
      test: curl -f http://localhost:8080/api/v1/health || exit 1
      interval: 5s
      retries: 10

  # Stream Processor Spring Boot
  stream-processor:
    build:
      context: ./stream-processor-spring
      dockerfile: Dockerfile
    container_name: int-test-stream-processor
    ports:
      - "8083:8080"
    environment:
      # Confluent Cloud configuration (from environment variables)
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_API_KEY: ${KAFKA_API_KEY:-}
      KAFKA_API_SECRET: ${KAFKA_API_SECRET:-}
      KAFKA_SECURITY_PROTOCOL: ${KAFKA_SECURITY_PROTOCOL:-SASL_SSL}
      KAFKA_SASL_MECHANISM: ${KAFKA_SASL_MECHANISM:-PLAIN}
      SPRING_KAFKA_STREAMS_SOURCE_TOPIC: raw-event-headers
      FILTER_CONFIG_PATH: /app/config/filters.json
    volumes:
      - ./config:/app/config:ro
      - ./stream-processor-spring/src/main/resources/filters.yml:/app/filters.yml:ro
    healthcheck:
      test: curl -f http://localhost:8080/actuator/health || exit 1
      interval: 5s
      retries: 10

  # V2 Stream Processor (for breaking schema changes)
  stream-processor-v2:
    build:
      context: ./stream-processor-spring
      dockerfile: Dockerfile
    container_name: int-test-stream-processor-v2
    profiles: ["v2"]
    depends_on:
      metadata-service:
        condition: service_healthy
    ports:
      - "8084:8080"
    environment:
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_API_KEY: ${KAFKA_API_KEY:-}
      KAFKA_API_SECRET: ${KAFKA_API_SECRET:-}
      KAFKA_SECURITY_PROTOCOL: ${KAFKA_SECURITY_PROTOCOL:-SASL_SSL}
      KAFKA_SASL_MECHANISM: ${KAFKA_SASL_MECHANISM:-PLAIN}
      SPRING_KAFKA_STREAMS_SOURCE_TOPIC: raw-event-headers-v2
      SPRING_KAFKA_STREAMS_APPLICATION_ID: event-stream-processor-v2
      FILTER_CONFIG_PATH: /app/config/filters-v2.json
      FILTER_VERSION: v2
      OUTPUT_TOPIC_SUFFIX: -v2
      # Enable Metadata Service integration for V2
      METADATA_SERVICE_ENABLED: "true"
      METADATA_SERVICE_URL: http://metadata-service:8080
      METADATA_SERVICE_SCHEMA_VERSION: v2
      METADATA_SERVICE_POLL_INTERVAL: 30000
    volumes:
      - ./config:/app/config:ro
      - ./stream-processor-spring/src/main/resources/filters.yml:/app/filters.yml:ro
    healthcheck:
      test: curl -f http://localhost:8080/actuator/health || exit 1
      interval: 5s
      retries: 10

  # Consumers (Spring variant - reads from -spring topics)
  car-consumer:
    build:
      context: ./consumers-confluent/car-consumer
    container_name: int-test-car-consumer
    environment:
      # Confluent Cloud configuration (from environment variables)
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_API_KEY: ${KAFKA_API_KEY:-}
      KAFKA_API_SECRET: ${KAFKA_API_SECRET:-}
      KAFKA_TOPIC: filtered-car-created-events-spring
      CONSUMER_GROUP_ID: int-test-car-consumer-group

  loan-consumer:
    build:
      context: ./consumers-confluent/loan-consumer
    container_name: int-test-loan-consumer
    environment:
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_API_KEY: ${KAFKA_API_KEY:-}
      KAFKA_API_SECRET: ${KAFKA_API_SECRET:-}
      KAFKA_TOPIC: filtered-loan-created-events-spring
      CONSUMER_GROUP_ID: int-test-loan-consumer-group

  loan-payment-consumer:
    build:
      context: ./consumers-confluent/loan-payment-consumer
    container_name: int-test-loan-payment-consumer
    environment:
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_API_KEY: ${KAFKA_API_KEY:-}
      KAFKA_API_SECRET: ${KAFKA_API_SECRET:-}
      KAFKA_TOPIC: filtered-loan-payment-submitted-events-spring
      CONSUMER_GROUP_ID: int-test-loan-payment-consumer-group

  service-consumer:
    build:
      context: ./consumers-confluent/service-consumer
    container_name: int-test-service-consumer
    environment:
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_API_KEY: ${KAFKA_API_KEY:-}
      KAFKA_API_SECRET: ${KAFKA_API_SECRET:-}
      KAFKA_TOPIC: filtered-service-events-spring
      CONSUMER_GROUP_ID: int-test-service-consumer-group

  # V2 Consumers (for breaking schema changes)
  car-consumer-v2:
    build:
      context: ./consumers-confluent/car-consumer
    container_name: int-test-car-consumer-v2
    profiles: ["v2"]
    environment:
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_API_KEY: ${KAFKA_API_KEY:-}
      KAFKA_API_SECRET: ${KAFKA_API_SECRET:-}
      KAFKA_TOPIC: filtered-car-created-events-v2-spring
      CONSUMER_GROUP_ID: int-test-car-consumer-group-v2

  loan-consumer-v2:
    build:
      context: ./consumers-confluent/loan-consumer
    container_name: int-test-loan-consumer-v2
    profiles: ["v2"]
    environment:
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_API_KEY: ${KAFKA_API_KEY:-}
      KAFKA_API_SECRET: ${KAFKA_API_SECRET:-}
      KAFKA_TOPIC: filtered-loan-created-events-v2-spring
      CONSUMER_GROUP_ID: int-test-loan-consumer-group-v2

  loan-payment-consumer-v2:
    build:
      context: ./consumers-confluent/loan-payment-consumer
    container_name: int-test-loan-payment-consumer-v2
    profiles: ["v2"]
    environment:
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_API_KEY: ${KAFKA_API_KEY:-}
      KAFKA_API_SECRET: ${KAFKA_API_SECRET:-}
      KAFKA_TOPIC: filtered-loan-payment-submitted-events-v2-spring
      CONSUMER_GROUP_ID: int-test-loan-payment-consumer-group-v2

  service-consumer-v2:
    build:
      context: ./consumers-confluent/service-consumer
    container_name: int-test-service-consumer-v2
    profiles: ["v2"]
    environment:
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_API_KEY: ${KAFKA_API_KEY:-}
      KAFKA_API_SECRET: ${KAFKA_API_SECRET:-}
      KAFKA_TOPIC: filtered-service-events-v2-spring
      CONSUMER_GROUP_ID: int-test-service-consumer-group-v2

  # Note: Topics are created in Confluent Cloud using Confluent CLI
  # See run-all-integration-tests.sh for topic creation via confluent kafka topic create

volumes:
  postgres-data:


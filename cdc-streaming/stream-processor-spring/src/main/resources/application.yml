spring:
  application:
    name: stream-processor-spring
  
  config:
    import: optional:file:./filters.yml
    activate:
      on-profile: default
  
  kafka:
    streams:
      application-id: ${SPRING_KAFKA_STREAMS_APPLICATION_ID:event-stream-processor-v1}
      bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
      source-topic: ${SPRING_KAFKA_STREAMS_SOURCE_TOPIC:raw-event-headers}
      properties:
        # Default to PLAINTEXT for local Redpanda, use SASL_SSL for Confluent Cloud
        security.protocol: ${KAFKA_SECURITY_PROTOCOL:PLAINTEXT}
        sasl.mechanism: ${KAFKA_SASL_MECHANISM:PLAIN}
        sasl.jaas.config: >
          org.apache.kafka.common.security.plain.PlainLoginModule required
          username="${KAFKA_API_KEY:}" password="${KAFKA_API_SECRET:}";
        # Exactly-once semantics
        processing.guarantee: exactly_once_v2
        # Consumer settings
        default.key.serde: org.apache.kafka.common.serialization.Serdes$StringSerde
        default.value.serde: com.example.streamprocessor.serde.EventHeaderSerde
        # Producer settings
        acks: all
        retries: 2147483647
        max.in.flight.requests.per.connection: 5
        enable.idempotence: true
        # State store settings
        state.dir: /tmp/kafka-streams
        # Replication factor for internal topics
        # For local Redpanda (single node): use 1, for Confluent Cloud: use 3
        replication.factor: ${KAFKA_REPLICATION_FACTOR:1}
        min.insync.replicas: ${KAFKA_MIN_INSYNC_REPLICAS:1}
        # Consumer settings for Confluent Cloud
        consumer.auto.offset.reset: earliest
        # Allow topic auto-creation (for local development)
        # Note: In production, topics should be created beforehand
        topic.allow.auto.create: true

management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics
  endpoint:
    health:
      show-details: always
  health:
    kafka:
      enabled: true
    kafkastreams:
      enabled: true

logging:
  level:
    com.example.streamprocessor: INFO
    org.apache.kafka: INFO
    org.apache.kafka.streams: INFO
    org.springframework.kafka: INFO
    org.springframework.kafka.streams: INFO

app:
  display-timezone: ${DISPLAY_TIMEZONE:America/New_York}

# Metadata Service configuration for dynamic filter loading
metadata:
  service:
    enabled: ${METADATA_SERVICE_ENABLED:false}
    url: ${METADATA_SERVICE_URL:http://localhost:8080}
    schema:
      version: ${METADATA_SERVICE_SCHEMA_VERSION:v1}
    poll:
      interval: ${METADATA_SERVICE_POLL_INTERVAL:30000}
    timeout-seconds: ${METADATA_SERVICE_TIMEOUT_SECONDS:5}
    max-retries: ${METADATA_SERVICE_MAX_RETRIES:3}
